# -*- coding: utf-8 -*-
"""Cloud Function.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zBU4Pic8jRFxnniHe1-qkN2JPckE9xRX
"""

#LA BUENA
import logging
import traceback
from flask import jsonify, request
from google.cloud import storage
import numpy as np
import pandas as pd
from datetime import datetime
import tensorflow as tf
from tensorflow.keras.models import load_model
from transformers import DistilBertTokenizer, TFDistilBertModel
from transformers import BertTokenizer, TFBertModel
from sklearn.preprocessing import StandardScaler
import plotly.express as px
from sklearn.metrics.pairwise import cosine_similarity
import matplotlib.pyplot as plt
import pickle
import zipfile
import os
import openai
from openai import OpenAI


# Configuración de logging
# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
# Configurar un manejador de logs explícito
root_logger = logging.getLogger()
root_logger.setLevel(logging.INFO)

# Añadir un StreamHandler para enviar los logs a la consola
if not root_logger.handlers:
    handler = logging.StreamHandler()
    handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))
    root_logger.addHandler(handler)


# Definir variables globales
bucket_name = 'bucket12project'

archivos = [
        'modelo_tiny/TMDB_tv_dataset_tiny.csv',
        'modelo_tiny/best_model_tiny.keras',
        'modelo_tiny/tinybert_model.zip',
        'modelo_tiny/tinybert_tokenizer.zip',
        'modelo_tiny/series_embeddings_tiny.pkl'
    ]


nombres_finales = []

# Variables para almacenar los modelos y datos cargados
modelo_keras = None
df = None
predictions = None
tokenizer = None
distilbert_model = None


def sp_to_en(contenido):
    OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY')
    client = OpenAI(api_key=OPENAI_API_KEY,)
    mensajes = [
        {"role": "system", "content": "Eres un asistente que ayuda a traducir texto de español a inglés"},
        {"role": "user", "content": f"Traduce de la manera más fiel y exacta posible el siguiente texto de español a inglés, si está en inglés no hagas nada: ({contenido}). Solo responde con la traducción en inglés, sin explicar nada"}
    ]

    # Crear una solicitud de finalización de chat
    respuesta = client.chat.completions.create(
        model="gpt-4-turbo",
        messages=mensajes
    )

    sust_dd= respuesta.choices[0].message.content

    return sust_dd


def en_to_sp(content):
    OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY')
    client = OpenAI(api_key=OPENAI_API_KEY,)

    messages = [
        {"role": "system", "content": "Eres un asistente que ayuda a traducir texto de inglés a español"},
        {"role": "user", "content": f"Traduce de la manera más fiel y exacta posible el siguiente texto de inglés a español, si está en español no hagas nada: ({content}). Solo responde con la traducción en español, sin explicar nada"}
    ]

    # Crear una solicitud de finalización de chat
    response = client.chat.completions.create(
        model="gpt-4-turbo",
        messages=messages
    )

    translated_content = response.choices[0].message.content

    return translated_content

def get_categorias():
    logging.info("Obteniendo categorías de series.")
    return ["Action & Adventure", "Animation", "Comedy", "Crime",
            "Documentary", "Drama", "Family", "History", "Kids", "Music",
            "Musical", "Mystery", "News", "Reality", "Romance", "Sci-Fi & Fantasy",
            "Soap", "Talk", "War & Politics", "Western"]


## ----------------------VERSIONES PREVIAS
def download_blob(bucket_name, source_blob_name, destination_file_name):
    """Downloads a blob from the bucket."""
    logging.info(f"Descargando blob {source_blob_name} desde el bucket {bucket_name}.")
    storage_client = storage.Client()
    bucket = storage_client.bucket(bucket_name)
    blob = bucket.blob(source_blob_name)
    blob.download_to_filename(destination_file_name)
    logging.info(f"Archivo descargado en {destination_file_name}.")


def descargar_todos_los_archivos(archivos, bucket_name):

    final_names = []
    for archivo in archivos:
        destino = f"/tmp/{archivo.split('/')[-1]}"
        logging.info(f"Descargando archivo {archivo} desde el bucket {bucket_name}.")
        download_blob(bucket_name, archivo, destino)
        final_names.append(destino)

    return final_names


def descomprimir_archivo_zip(archivo_zip, destino):
    try:
        with zipfile.ZipFile(archivo_zip, 'r') as zip_ref:
            zip_ref.extractall(destino)
        logging.info(f"Archivo {archivo_zip} descomprimido en {destino}.")
    except Exception as e:
        logging.error(f"Error al descomprimir {archivo_zip}: {e}")

def procesar_archivos(nombres_finales):
    try:
        # Cargar el modelo Keras
        modelo_keras = load_model(nombres_finales[1])
        logging.info("Modelo Keras cargado exitosamente.")
    except Exception as e:
        logging.error(f"Error al cargar el modelo Keras: {e}")
        modelo_keras = None

    try:
        # Leer el archivo CSV con pandas
        df = pd.read_csv(nombres_finales[0])
        logging.info("CSV cargado exitosamente.")
    except Exception as e:
        logging.error(f"Error al leer el CSV: {e}")
        df = None

    try:
        # Cargar el archivo pickle
        with open(nombres_finales[4], 'rb') as f:
            predictions = pickle.load(f)
        logging.info("Archivo pickle cargado exitosamente.")
    except Exception as e:
        logging.error(f"Error al cargar el archivo pickle: {e}")
        predictions = None

    try:
        # Descomprimir el archivo del tokenizer
        tokenizer_zip_destino = "/tmp/tokenizer"
        descomprimir_archivo_zip(nombres_finales[3], tokenizer_zip_destino)

        # Cargar el tokenizer DistilBERT desde el destino descomprimido
        tokenizer = BertTokenizer.from_pretrained(tokenizer_zip_destino, local_files_only=True)
        logging.info("Tokenizer DistilBERT cargado exitosamente.")
    except Exception as e:
        logging.error(f"Error al cargar el tokenizer DistilBERT: {e}")
        tokenizer = None

    try:
        # Descomprimir el archivo del modelo DistilBERT
        distilbert_model_zip_destino = "/tmp/distilbert-base-uncased"
        descomprimir_archivo_zip(nombres_finales[2], distilbert_model_zip_destino)

        # Verificar que el archivo se descomprimió correctamente
        if os.path.exists(distilbert_model_zip_destino):
            logging.info(f"Archivos descomprimidos en {distilbert_model_zip_destino}: {os.listdir(distilbert_model_zip_destino)}")
        else:
            logging.error(f"No se encontró el directorio {distilbert_model_zip_destino} después de la descompresión.")
            raise FileNotFoundError(f"El directorio {distilbert_model_zip_destino} no se encuentra.")
    except Exception as e:
        logging.error(f"Error al descomprimir el modelo DistilBERT: {e}")
        distilbert_model = None

    # Comprobación adicional antes de cargar el modelo
    logging.error(f"Ahí va el Error")

    try:
        # Cargar el modelo DistilBERT desde el destino descomprimido (sin from_pt=True si es TensorFlow)
        distilbert_model = TFBertModel.from_pretrained(distilbert_model_zip_destino, local_files_only=True)
        logging.info("Modelo DistilBERT cargado exitosamente.")
    except Exception as e:
        logging.error(f"Error al cargar el modelo DistilBERT: {e}")
        logging.error("Detalle completo del error: " + str(traceback.format_exc()))
        try:
            logging.info(f"Intentando cargar el modelo DistilBERT desde local_files_only False")
            # Cargar el modelo DistilBERT desde el destino descomprimido (sin from_pt=True si es TensorFlow)
            distilbert_model = TFBertModel.from_pretrained(distilbert_model_zip_destino)
            logging.info("Modelo DistilBERT cargado exitosamente desde local_files_only False.")

        except:
            logging.info(f"Intentando cargar el modelo DistilBERT desde huggingface")
            # Cargar el modelo DistilBERT desde el destino descomprimido
            distilbert_model = TFBertModel.from_pretrained('huawei-noah/TinyBERT_General_4L_312D')#distilbert-base-uncased
            logging.info("Modelo DistilBERT cargado exitosamente desde huggingface.")


    return modelo_keras, df, predictions, tokenizer, distilbert_model


def get_distilbert_embeddings(texts, distilbert_model, tokenizer, batch_size=8):
    embeddings = []
    for i in range(0, len(texts), batch_size):
        batch_texts = texts[i:i + batch_size]
        inputs = tokenizer(batch_texts, return_tensors="tf", padding=True, truncation=True, max_length=512)
        outputs = distilbert_model(inputs)
        batch_embeddings = outputs.last_hidden_state[:, 0, :].numpy()
        embeddings.append(batch_embeddings)
    return np.vstack(embeddings)

def recommend_similar_series(new_serie_plot, model, df, predictions, distilbert_model, tokenizer, top_n=3):

    categories = get_categorias()

    try:
        logging.info("Obteniendo embeddings para la nueva serie")
        new_serie_embedding = get_distilbert_embeddings([new_serie_plot.lower()], distilbert_model, tokenizer, batch_size=1)
        logging.info("Embeddings obtenidos correctamente")

        logging.info("Generando puntuaciones de género para la nueva serie")
        new_serie_scores = model.predict(new_serie_embedding)
        logging.info("Puntuaciones de género generadas")

        # Calcular similitudes con otras series
        logging.info("Calculando similitudes con otras series")
        similarities = cosine_similarity(new_serie_scores, predictions).flatten()
        similarity_df = df.copy()
        similarity_df['similarity'] = similarities
        similar_series = similarity_df.sort_values(by='similarity', ascending=False).head(top_n)
        logging.info(f"Similitudes calculadas, top {top_n} series similares seleccionadas")

        # Crear lista de recomendaciones para el retorno en JSON
        recommendations = []
        logging.info("Generando lista de recomendaciones")
        for i, row in similar_series.iterrows():
            real_genres = [genre for genre in categories if row[genre] == 1]
            recommendations.append({
                'name': row['name'],
                'synopsis': row['processed_plot'],
                'genres': real_genres,
                'similarity': round(row['similarity'], 2)
            })
        logging.info("Recomendaciones generadas correctamente")

    except Exception as e:
        logging.error(f"Error en recommend_similar_series: {str(e)}")
        raise e  # Volver a lanzar la excepción para manejo fuera de la función si es necesario

    return recommendations

def format_recommendations_to_string(recommendations):
    output = []
    intro = 'De acuerdo a los gustos que me mencionaste, estoy seguro que las siguientes series te gustarán: '
    for recomendacion in recommendations:
        genres = ', '.join(recomendacion['genres'])
        formatted = (
            f'''Nombre: {recomendacion['name']}\nSinopsis: {recomendacion['synopsis']}
            '''
        )
        output.append(formatted)

    output_en = "\n".join(output)
    output_es = en_to_sp(output_en)

    return intro+output_es


def get_genre_prediction_data(new_serie_plot, model, distilbert_model, tokenizer):
    # Obtener embeddings y puntuaciones de género
    new_serie_embedding = get_distilbert_embeddings([new_serie_plot.lower()], distilbert_model, tokenizer, batch_size=1)
    new_serie_scores = model.predict(new_serie_embedding)
    genre_scores = new_serie_scores[0] * 100  # Multiplicar por 100 las puntuaciones

    categories = get_categorias()

    # Filtrar las categorías y puntuaciones mayores a 0
    filtered_scores = [(category, int(score)) for category, score in
    zip(categories, genre_scores) if not np.isnan(score) and score > 0][:5]

    # Ordenar por puntuación de mayor a menor
    filtered_scores = sorted(filtered_scores, key=lambda x: x[1], reverse=True)
    categories_sorted, scores_sorted = zip(*filtered_scores)

    # Retornar los datos como un diccionario
    return {
        'categories': list(categories_sorted),
        'scores': list(scores_sorted)
    }


### VERSION ANTERIOR:
def predict_function_gen2_tiny_0(request_resp):
    logging.info("Iniciando la función predict_function.")

    request_resp_json = request_resp.get_json()

    if not request_resp_json or 'x' not in request_resp_json or 'y' not in request_resp_json:
        logging.error("Solicitud incorrecta: No se proporcionaron valores de 'x' o 'y'.")
        return jsonify({"error": "Bad request_resp: No x or y provided"}), 400

    logging.info("Datos recibidos: "+str(request_resp_json['x']))
    x = request_resp_json['x']

    x = sp_to_en(x)
    logging.info("Datos traducidos: "+str(x))

    y = request_resp_json['y']

    request_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    client_ip = request.headers.get('X-Forwarded-For', request.remote_addr)

    logging.info(f"Momento de la solicitud: {request_time}")
    logging.info(f"IP del cliente: {client_ip}")
    logging.info("")


    # Definir nombre del bucket y archivo
    bucket_name = 'bucket12project'

    archivos = [
        'modelo_tiny/TMDB_tv_dataset_tiny.csv',
        'modelo_tiny/best_model_tiny.keras',
        'modelo_tiny/tinybert_model.zip',
        'modelo_tiny/tinybert_tokenizer.zip',
        'modelo_tiny/series_embeddings_tiny.pkl'
    ]



    nombres_finales = descargar_todos_los_archivos(archivos, bucket_name)

    recomendaciones_str = ''

    try:
        logging.info("Iniciando el procesamiento de archivos")
        modelo_keras, df, predictions, tokenizer, distilbert_model = procesar_archivos(nombres_finales)

        logging.info("Archivos procesados correctamente")

        try:
            logging.info("Iniciando la recomendación de series similares")
            recomendaciones = recommend_similar_series(str(x),
                                                    modelo_keras,
                                                    df,
                                                    predictions,
                                                    distilbert_model,
                                                    tokenizer, top_n=3)

            recomendaciones_str = format_recommendations_to_string(recomendaciones)
            logging.info("Recomendación generada exitosamente")

        except Exception as e:
            recomendaciones_str = 'No se logró la recomendación'
            logging.error(f"Error en la recomendación: {str(e)}")

    except Exception as e:
        framew = ' No se logró'
        logging.error(f"Error en el procesamiento de archivos: {str(e)}")


    try:
        logging.info("Iniciando la obtención de datos de predicción de género")
        # Obtener los datos de categorías y puntuaciones
        genre_data = get_genre_prediction_data(str(x), modelo_keras, distilbert_model, tokenizer)
        categories = genre_data['categories']
        scores = genre_data['scores']
        logging.info("Datos de predicción de género obtenidos exitosamente")

    except Exception as e:
        categories = []
        scores = []
        logging.error(f"Error en la obtención de datos de predicción de género: {str(e)}")

    # salida = ' '.join([pal + '' for pal in x.split(' ')])

    salida = recomendaciones_str
    logging.info(f"Procesamiento de la salida basado en 'x': {salida}")

    # A partir de aquí empieza el uso de modelos, ETL, etc.
    logging.info("Finalizando la función predict_function.")

    return jsonify({
        'respuesta': salida,
        'categories': categories,
        'scores': scores,
        'momento': request_time,
        'cliente_ip': client_ip
    })
